{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking notebook\n",
    "\n",
    "This notebook contains all of the methods and analysis for performing benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook parameters\n",
    "These parameters are used throughout the notebook for benchmarking. These parameters include paths, tuning, and other various parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_DIR = Path(\"/Users/dreyceyalbin/Desktop/Phage-Enrich\")\n",
    "if not os.path.isdir(FIGURE_DIR): os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "file_with_names = \"./abundance.tsv\"\n",
    "accession2taxid_file = \"../../database/krakenDB/taxonomy/nucl_gb.accession2taxid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers\n",
    "These methods are used for parsing all of the tools being benchmarked. These parsers are passed to a general parser function, giving a strategy-pattern-like method for obtaining result from tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8957\n"
     ]
    }
   ],
   "source": [
    "def get_needed_ncbi_ids():\n",
    "    \"\"\"\n",
    "    gets NCBI ids\n",
    "    \"\"\"\n",
    "    file_with_names_opened = open(file_with_names)\n",
    "    \n",
    "    # grab all needed NCBI ids\n",
    "    ncbi_ids = []\n",
    "    line = file_with_names_opened.readline()\n",
    "    line_counter = 0\n",
    "    while(line):\n",
    "        if (line_counter > 0): # skip header\n",
    "            ncbi_id = line.split(\"\\t\")[0]\n",
    "            ncbi_ids.append(ncbi_id)\n",
    "        \n",
    "        line = file_with_names_opened.readline()\n",
    "        line_counter += 1\n",
    "        \n",
    "    file_with_names_opened.close()\n",
    "    \n",
    "    return ncbi_ids\n",
    "\n",
    "ncbi_ids = get_needed_ncbi_ids()\n",
    "print(len(ncbi_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping_dictionary():\n",
    "    \"\"\"\n",
    "    Creates dictionary for mapping NCBI ids to \n",
    "    taxonomy.\n",
    "    \n",
    "    Uses nucl_gb.accession2taxid\n",
    "    \n",
    "    NC_033618\tNC_033618.1\t1931113\t1139918407\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "\n",
    "    # grab taxid mappings\n",
    "    acc2tax_open = open(accession2taxid_file)\n",
    "    line = acc2tax_open.readline()\n",
    "\n",
    "    while(line):\n",
    "        line = acc2tax_open.readline()\n",
    "        line = line.split(\"\\t\")\n",
    "        accession = line[1]\n",
    "        taxid = line[2]\n",
    "        if accession in ncbi_ids:\n",
    "            dictionary[accession] = taxid\n",
    "    acc2tax_open.close()\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "accession2taxid = create_mapping_dictionary()\n",
    "\n",
    "print(accession2taxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_enrichseq(debExt_dir, extension=\"ss3\", non_test=False, truth_csv=None):\n",
    "    \"\"\"\n",
    "    This function parses the debext output directory.\n",
    "    OUTPUT:\n",
    "        dict = {}\n",
    "    \"\"\"\n",
    "    prediction_outcomes = {} # taxid : abundance\n",
    "    if truth_csv != None: truth_dict = parse_truth(truth_csv)\n",
    "    for outputfile in glob.glob(debExt_dir+'/*'+extension): #os.listdir(debExt_dir): \n",
    "        with open(Path(outputfile)) as resultsfile:\n",
    "            resultlines = resultsfile.readlines()\n",
    "            outputfile = Path(outputfile)\n",
    "            for line_index, line in enumerate(resultlines):\n",
    "                if line[0] == \">\":\n",
    "                    if non_test:\n",
    "                        identifier = outputfile.name.split(\".\")[0]\n",
    "                    else:\n",
    "                        identifier = line.strip(\">\").strip(\"\\n\") # uses file name.\n",
    "                    if truth_csv != None:\n",
    "                        prediction_outcomes[identifier] = truth_dict[identifier]\n",
    "                    else:\n",
    "                        prediction_outcomes[identifier] = resultlines[line_index+2].strip(\"\\n\")\n",
    "    return prediction_outcomes\n",
    "\n",
    "def parser_fastviromeexplorer(directory, extension=\"ss3\", non_test=False):\n",
    "    \"\"\"\n",
    "    This function parses the debext output directory.\n",
    "    OUTPUT:\n",
    "        dict = {}\n",
    "    \"\"\"\n",
    "    prediction_outcomes = {} # taxid : abundance\n",
    "\n",
    "    return prediction_outcomes\n",
    "\n",
    "def ncbi2taxid(ncbi, dictionary):\n",
    "    \"\"\"\n",
    "    \n",
    "    uses nucl_gb.accession2taxid\n",
    "    dictionary(NC_033618.1) -> \n",
    "    1. get the names of each NCBI\n",
    "    \"\"\"\n",
    "    return dictionary[ncbi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Methods and structures\n",
    "The methods and data structures here are used for creating a common data structure for the output of all tools being compared with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultStruct:\n",
    "    \"\"\" datastruct for holding results \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.taxid2abundance : Dict = {}\n",
    "        self.taxid2length : Dict = {}\n",
    "        self.conf_matrix = np.zeros((2,2))\n",
    "        self.precision = {}\n",
    "        self.recall = {}\n",
    "        #self.time : Dict[str, Dict[str,float]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(directory, truth_csv, parser_function):\n",
    "    \"\"\"\n",
    "    This function parses all files within an output\n",
    "    folder for DebruijnExtend.\n",
    "\n",
    "    OUTPUT:                                                                     \n",
    "        results = { \"accuracy\" : [[.8, .4, ..., 0.7], # percent guessed correctly  \n",
    "                    \"length\" : [254, 223, ..., 30], # length per                \n",
    "                    \"conf_matrix\" : [[],[],[]] # confusion matrix}\n",
    "    \"\"\"\n",
    "    #results_dict = {test_iteration : [[], [], np.zeros((3,3))] }\n",
    "    results: ResultStruct = ResultStruct()\n",
    "    seq_dict = parser_function(directory) ### CHANGE\n",
    "    truth_dict = parse_truth(truth_csv)\n",
    "    results.time = get_times(directory)\n",
    "    for identifier, prediction in seq_dict.items():\n",
    "        true_ss3 = truth_dict[identifier]\n",
    "        accuracy, confusion_matrix = prediction_rank(prediction, true_ss3)\n",
    "        # append results to dictinoary\n",
    "        results.accuracy.append(accuracy)\n",
    "        results.length.append(len(true_ss3))\n",
    "        results.pdb2length[identifier] = len(true_ss3)\n",
    "        results.conf_matrix += confusion_matrix\n",
    "        results.pdb_names.append(identifier)\n",
    "    return results\n",
    "    \n",
    "def parse_truth(truth_csv, parsing_index=3):\n",
    "    \"\"\"\n",
    "    parses the truth CSV to get the actual ss3\n",
    "    \"\"\"\n",
    "    truth_dict = {}\n",
    "    with open(truth_csv) as file_by_lines:\n",
    "        file_lines = file_by_lines.readlines()\n",
    "        for index, line in enumerate(file_lines):\n",
    "            identifier = line.split(\",\")[parsing_index-2].strip(\"\\n\").strip(\"'\")\n",
    "            truth_dict[identifier] = line.split(\",\")[parsing_index].strip(\"\\n\").strip(\"'\") #TODO: ASSUMES SAME ORDER AS FASTA!!\n",
    "    return truth_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Methods\n",
    "These methods use the common `ResultStruct` datastructure (or arry thereof) for plotting individual tool metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(input_results: ResultStruct, \n",
    "                          fig_path=None, \n",
    "                          title: str=None):                                                        \n",
    "    \"\"\"                                                                         \n",
    "    This function plot the confusion matrix \n",
    "    \"\"\" \n",
    "    total_confusion = input_results.conf_matrix\n",
    "    for row_ind, row in enumerate(total_confusion):\n",
    "        total_confusion[row_ind] /= np.sum(total_confusion[row_ind])\n",
    "    print(total_confusion)\n",
    "    # turn array into pandas dataframe\n",
    "    total_confusion = pd.DataFrame(total_confusion, \n",
    "                                   columns=['C', 'E', 'H'], \n",
    "                                   index=['C', 'E', 'H'])\n",
    "    ## plot confusion matrix\n",
    "    ax = sns.heatmap(total_confusion, annot=True, linewidths=.5, cmap='binary')\n",
    "    if title: plt.title(title)\n",
    "    if fig_path:\n",
    "        plt.savefig(fig_path, transparent=True, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "This section splits the various benchmarking sections into a clear, concise set of scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of genomes comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of read mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
