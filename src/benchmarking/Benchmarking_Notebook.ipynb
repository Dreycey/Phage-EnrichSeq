{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking notebook\n",
    "\n",
    "This notebook contains all of the methods and analysis for performing benchmarking.\n",
    "\n",
    "Each section contains a description to allow for more-easily walking through the code. We keep the benchmarking code in a jupyter-notebook as we antipate ourselves (and others) adding new parsers and using this for replicating/extending benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std library\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import json\n",
    "# non-std library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook parameters\n",
    "These parameters are used throughout the notebook for benchmarking. These parameters include paths, tuning, and other various parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_names = \"./abundance_new.tsv\"\n",
    "accession2taxid_file_path = \"../../database/krakenDB/taxonomy/nucl_gb.accession2taxid\"\n",
    "ncbi_map_file_path = \"ncbi_map.tab\"\n",
    "ENRICHSEQ_OUTPUTILE = None # allows for switching files to parse for enrichseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ncbi_map_file_path):\n",
    "    !cat ../../database/krakenDB/taxonomy/nucl_gb.accession2taxid | grep \"NC_\" > ncbi_map.tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mapping dictionary\n",
    "This step is used for creating a map from NCBI IDs to taxonomy IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_needed_ncbi_ids():\n",
    "    \"\"\"\n",
    "    **DEPRECATED FUNCTION**\n",
    "        was originally used to get NCBI ids from an output\n",
    "        fastviromexplorer file.\n",
    "    **DEPRECATED FUNCTION**\n",
    "    \"\"\"\n",
    "    file_with_names_opened = open(file_with_names)\n",
    "    \n",
    "    # grab all needed NCBI ids\n",
    "    ncbi_ids = []\n",
    "    line = file_with_names_opened.readline()\n",
    "    line_counter = 0\n",
    "    while(line):\n",
    "        if (line_counter > 0): # skip header\n",
    "            ncbi_id = line.split(\"\\t\")[0]\n",
    "            ncbi_ids.append(ncbi_id)\n",
    "        line = file_with_names_opened.readline()\n",
    "        line_counter += 1\n",
    "        \n",
    "    file_with_names_opened.close()\n",
    "    \n",
    "    return ncbi_ids\n",
    "\n",
    "ncbi_ids = get_needed_ncbi_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping_dictionary(map_file=ncbi_map_file_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Creates dictionary for mapping NCBI ids to \n",
    "        taxonomy.\n",
    "    Notes:\n",
    "        1. cat nucl_gb.accession2taxid | grep \"NC_\" > ncbi_ids.tab\n",
    "        2. Uses ncbi_ids.tab\n",
    "    Output:\n",
    "        NC_033618\tNC_033618.1\t1931113\t1139918407\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    # grab taxid mappings\n",
    "    acc2tax_open = open(map_file)\n",
    "    line = acc2tax_open.readline()\n",
    "    counter = 0\n",
    "    while(line):\n",
    "        line = line.split(\"\\t\")\n",
    "        accession = line[0]\n",
    "        taxid = line[2]\n",
    "        dictionary[accession] = taxid\n",
    "        counter += 1\n",
    "#         if (counter % 10000 == 0):\n",
    "#             print(f\"{counter} lines have been parsed \\n\")\n",
    "        line = acc2tax_open.readline()\n",
    "    acc2tax_open.close()\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2919554'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test.\n",
    "dictionary = create_mapping_dictionary()\n",
    "dictionary['NC_014459']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers\n",
    "These methods are used for parsing all of the tools being benchmarked. These parsers are passed to a general parser function, giving a strategy-pattern-like method for obtaining result from tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_enrichseq(enrichseq_dir: Path, delim=\",\", use_merge_overlap=ENRICHSEQ_OUTPUTILE):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function parses the enrichseq output directory.\n",
    "        \n",
    "    Notes:\n",
    "        use 'taxid_abundances.csv' or 'merge_overlap_out_refined.csv'\n",
    "    Input:\n",
    "        1. Path to a singular EnrichSeq output directory.\n",
    "        2. [optional; Def=','] - delimiter\n",
    "        \n",
    "    Output:\n",
    "        Abundance Dictionary containing:\n",
    "            1. taxid abundances\n",
    "            abundances_dict = {'tax_id_1' : 33,\n",
    "                               'tax_id_2' : 33,\n",
    "                               ...,\n",
    "                               'tax_id_N' : 33,\n",
    "                              }\n",
    "            2. cluster abundances\n",
    "            abundances_dict = {'cluster_1' : 33,\n",
    "                               'cluster_2' : 33,\n",
    "                               ...,\n",
    "                               'cluster_M' : 33,\n",
    "                              }\n",
    "    \"\"\"\n",
    "    # nested dictionary of different abundance measurements\n",
    "    abundances = {\"taxid_abundance\" : {},\"cluster_abundance\" : {}} \n",
    "    # get paths\n",
    "    output_path = enrichseq_dir / Path('enrichseq/output_files/')\n",
    "    if use_merge_overlap:\n",
    "        taxid_abundances_path = output_path / Path('merge_overlap_out_refined.csv')\n",
    "    else:\n",
    "        taxid_abundances_path = output_path / Path('taxid_abundances.csv')\n",
    "    cluster_abundances_path = output_path / Path('cluster_abundances.csv')\n",
    "    # csvs to parse.\n",
    "    csvs_to_parse = {\"taxid_abundance\" : taxid_abundances_path, \n",
    "                     \"cluster_abundance\" : cluster_abundances_path}\n",
    "    # parse abundance CSVs\n",
    "    for csv_name, csv_file in csvs_to_parse.items():\n",
    "        with open(csv_file, \"r\") as csv_file_opened:\n",
    "            csv_file_lines = csv_file_opened.readlines() # never that big, read all into RAM\n",
    "            for line in csv_file_lines:\n",
    "                tax_clust_id, abundance_val = line.strip(\"\\n\").split(delim)\n",
    "                abundances[csv_name][tax_clust_id] = float(abundance_val)\n",
    "\n",
    "    return abundances\n",
    "\n",
    "def parser_fastviromeexplorer(directory, name_column=0, counts_column=3, delim=\"\\t\"):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        This function parses the FastViromeExplorer output directory.\n",
    "        \n",
    "    Input:\n",
    "        1. Path to a singular FastViromeExplorer output directory.\n",
    "        2. column index corresponding to NCBI id\n",
    "        3. column index corresponding to the read counts\n",
    "        4. [optional; Def='\\t'] - delimiter\n",
    "        \n",
    "    Output:\n",
    "        Abundance Dictionary containing:\n",
    "            1. taxid abundances\n",
    "            abundances_dict = {'tax_id_1' : 33,\n",
    "                               'tax_id_2' : 33,\n",
    "                               ...,\n",
    "                               'tax_id_N' : 33,\n",
    "                              }\n",
    "    \"\"\"\n",
    "    # init datastructures\n",
    "    abundances = {\"taxid_abundance\" : {}} # <tax/clust>id : abundance\n",
    "\n",
    "    # read in the NCBI to Tax id namings\n",
    "    with open('out_dictionary.txt') as f:\n",
    "        dictionary_str = f.read()\n",
    "    dictionary = json.loads(dictionary_str)\n",
    "    dictionary.update(make_dir())\n",
    "    dictionary.update(create_mapping_dictionary())\n",
    "    # get paths\n",
    "    taxid_abundances = directory / Path('abundance.tsv')\n",
    "    # parse abundance CSVs\n",
    "    csvs_to_parse = {\"taxid_abundance\" : taxid_abundances}\n",
    "    total_abundance = 0\n",
    "    # parsing \n",
    "    for csv_name, csv_file in csvs_to_parse.items():\n",
    "        with open(csv_file, \"r\") as csv_file_opened:\n",
    "            csv_file_lines = csv_file_opened.readlines() # never that big, read all into RAM\n",
    "            for line_ind, line in enumerate(csv_file_lines):\n",
    "                if line_ind > 0: # skip header\n",
    "                    ncbi_id = line.strip(\"\\n\").split(delim)[name_column]\n",
    "                    counts_val = line.strip(\"\\n\").split(delim)[counts_column]\n",
    "\n",
    "                    # convert NCBI name to taxid, save counts\n",
    "                    if ncbi_id in dictionary:\n",
    "                        tax_clust_id = dictionary[ncbi_id]\n",
    "                    elif ncbi_id.split(\".\")[0] in dictionary:\n",
    "                        tax_clust_id = dictionary[ncbi_id.split(\".\")[0]] # 'dictionary' imported above\n",
    "                    else:\n",
    "                        tax_clust_id = 'unk'\n",
    "                        if float(counts_val) > 0.01: #if above zero it may be a phage\n",
    "                            print(f\"ERROR (parsing):\")\n",
    "                            print(f\"\\tTool: FVE\")\n",
    "                            print(f\"\\t\\tFVE: {ncbi_id} NOT CONVERTED\")\n",
    "                            print(f\"\\t\\tabundances: {float(counts_val)}\")\n",
    "                    abundances[csv_name][tax_clust_id] = float(counts_val)\n",
    "                    total_abundance += float(counts_val)\n",
    "    # update abundances dictionary, take non less than threshold\n",
    "    ids_to_del = []\n",
    "    for tax_id, abundance_val in abundances[\"taxid_abundance\"].items():\n",
    "        if total_abundance > 0:\n",
    "            abundances[\"taxid_abundance\"][tax_id] /= total_abundance #normalize\n",
    "        if abundances[\"taxid_abundance\"][tax_id] < 0.001:\n",
    "            ids_to_del.append(tax_id)\n",
    "    # delete 0 value taxids\n",
    "    for val in ids_to_del:\n",
    "        del abundances[\"taxid_abundance\"][val]\n",
    "\n",
    "    return abundances\n",
    "\n",
    "def parser_bracken(directory, taxid_column=1, abundance_column=6, delim=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function parses the enrichseq output directory.\n",
    "        \n",
    "    Input:\n",
    "        1. Path to a singular Bracken output directory.\n",
    "        2. [optional; Def='\\t'] - delimiter\n",
    "        \n",
    "    Output:\n",
    "        Abundance Dictionary containing:\n",
    "            1. taxid abundances\n",
    "            abundances_dict = {'tax_id_1' : 33,\n",
    "                               'tax_id_2' : 33,\n",
    "                               ...,\n",
    "                               'tax_id_N' : 33,\n",
    "                              }\n",
    "    \"\"\"\n",
    "    # dictionary of bracken abundance \n",
    "    abundances = {'taxid_abundance' : {}}\n",
    "    # get paths\n",
    "    taxid_abundances = directory / Path('abundances.bracken')\n",
    "    dictionary = make_dir()\n",
    "    files_to_parse = {\"taxid_abundance\" : taxid_abundances}\n",
    "    \n",
    "    # parse bracken abundance files\n",
    "    for file_name, bracken_file in files_to_parse.items():\n",
    "        with open(bracken_file, \"r\") as bracken_file_opened:\n",
    "            bracken_file_lines = bracken_file_opened.readlines() # never that big, read all into RAM\n",
    "            for line_ind, line in enumerate(bracken_file_lines):\n",
    "                if line_ind > 0: # skip header\n",
    "#                     ncbi_id = line.strip(\"\\n\").split(delim)[name_column]\n",
    "#                     counts_val = line.strip(\"\\n\").split(delim)[counts_column]\n",
    "                    tax_id = line.strip(\"\\n\").split(delim)[taxid_column]\n",
    "                    abundance_val = line.strip(\"\\n\").split(delim)[abundance_column]\n",
    "                    abundances[file_name][tax_id] = float(abundance_val)\n",
    "    \n",
    "    # update abundances dictionary\n",
    "    ids_to_del = []\n",
    "    for tax_id, abundance_val in abundances[\"taxid_abundance\"].items():\n",
    "        if abundances[\"taxid_abundance\"][tax_id] < 0.001:\n",
    "            ids_to_del.append(tax_id)\n",
    "    # delete 0 value taxids\n",
    "    for val in ids_to_del:\n",
    "        del abundances[\"taxid_abundance\"][val]\n",
    "\n",
    "    return abundances\n",
    "\n",
    "def make_dir():\n",
    "    ncbi2taxid = \"//scratch/summit/dral3008/BENCHMARKING_RESULTS/database/krakenDB/seqid2taxid.map\"\n",
    "    dictionary = {}\n",
    "    with open(ncbi2taxid) as ncbi2taxid:\n",
    "        lines = ncbi2taxid.readlines()\n",
    "        for line in lines:\n",
    "            ncbi, taxid = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "            dictionary[ncbi] = taxid\n",
    "    return dictionary\n",
    "\n",
    "def parse_simulated_fasta(fasta_path):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        This function parses the simulated true fasta file used\n",
    "        for testing.\n",
    "        \n",
    "    Input:\n",
    "        1. Path to a singular simulated fasta file\n",
    "        \n",
    "    Output:\n",
    "        Abundance Dictionary containing:\n",
    "            1. taxid abundances\n",
    "            abundances_dict = {'tax_id_1' : 33,\n",
    "                               'tax_id_2' : 33,\n",
    "                               ...,\n",
    "                               'tax_id_N' : 33,\n",
    "                              }\n",
    "    \"\"\"\n",
    "    abundances = {'taxid_abundance' : {}}\n",
    "    total_counts = 0\n",
    "    with open('out_dictionary.txt') as f:\n",
    "        dictionary_str = f.read()\n",
    "    dictionary = json.loads(dictionary_str)\n",
    "    dictionary.update(make_dir())\n",
    "    dictionary.update(create_mapping_dictionary())\n",
    "    not_found = set()\n",
    "    prev_len = 0\n",
    "    with open(fasta_path, \"r\") as fasta_opened:\n",
    "        line = fasta_opened.readline()\n",
    "        while (line):\n",
    "            if (line[0] == \">\"):\n",
    "                ncbi_id = line[1:].split(\"|\")[0].split(\"-\")[0]\n",
    "                # dictioinary from FastViromeExplorer, obtained above\n",
    "                if ncbi_id in dictionary:\n",
    "                    name = dictionary[line[1:].split(\"|\")[0].split(\"-\")[0]]\n",
    "                else:\n",
    "                    file_name = line[1:].split(\"|\")[-1].strip(\"\\n\")\n",
    "                    path = Path(\"//scratch/summit/dral3008/BENCHMARKING_RESULTS/database/ref_genomes/\" + file_name)\n",
    "                    try:\n",
    "                        name = open(path).readline().split(\"kraken:taxid|\")[1].strip(\"\\n\").replace(\" \", \"\")\n",
    "                    except:\n",
    "                        not_found.add(path)\n",
    "                        new_len = len(not_found)\n",
    "                        if new_len != prev_len:\n",
    "                            print(f\"ERROR (parsing):\")\n",
    "                            print(f\"\\tTool: FVE\")\n",
    "                            print(f\"\\t\\tnumber of true NOT found (fine if 1): {new_len}\")\n",
    "                            print(f\"\\t\\tNCBI ID: {ncbi_id}\")\n",
    "                            prev_len = new_len\n",
    "                if name in abundances['taxid_abundance']:\n",
    "                    abundances['taxid_abundance'][name] += 1\n",
    "                else:\n",
    "                    abundances['taxid_abundance'][name] = 1\n",
    "                total_counts += 1\n",
    "            line = fasta_opened.readline()\n",
    "            \n",
    "    #turn counts into abundances - normalize\n",
    "    for tax_id in abundances['taxid_abundance'].keys():\n",
    "        abundances['taxid_abundance'][tax_id] /= total_counts #normalize\n",
    "    return abundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing parsing functions\n",
    "\n",
    "# # Paths\n",
    "# fastvirome_test_dir = \"//scratch/summit/dral3008/benchmarking_enrichseq/results/FastViromeExplorer/num_mutations_test/3_genomes_sim_05del_05ins_illumina/\"\n",
    "# enrichseq_test_dir = \"//scratch/summit/dral3008/benchmarking_enrichseq/results/enrichseq/num_mutations_test/3_genomes_sim_00del_05ins_illumina./\"\n",
    "# truth_test_dir = \"//scratch/summit/dral3008/benchmarking_enrichseq/tests/num_mutations_test/3_genomes_sim_05del_05ins_illumina.fa\"\n",
    "# # run tests\n",
    "# fastvirome_values = parser_fastviromeexplorer(fastvirome_test_dir)\n",
    "# enrichseq_values = parser_enrichseq(enrichseq_test_dir)\n",
    "# true_values = parse_simulated_fasta(truth_test_dir)\n",
    "# print(fastvirome_values)\n",
    "# print(enrichseq_values)\n",
    "# print(true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Methods and structures\n",
    "The methods and data structures here are used for creating a common data structure for the output of all tools being compared with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultStruct:\n",
    "    \"\"\" EnrichSeq - datastruct for holding results \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.l2_abundance_distance = []\n",
    "        self.classification_recall = 0\n",
    "        self.classification_precision = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSERS_MAP = {\"EnrichSeq_k75m31\" : parser_enrichseq,\n",
    "               \"EnrichSeq_k28m28\" : parser_enrichseq,\n",
    "               \"EnrichSeq_k35m31\" : parser_enrichseq,\n",
    "               \"EnrichSeq_k25\" : parser_enrichseq,\n",
    "               \"EnrichSeq\" : parser_enrichseq,\n",
    "               \"EnrichSeq_wMergeOverlap\" : parser_enrichseq,\n",
    "               \"FastViromeExplorer\" : parser_fastviromeexplorer,\n",
    "               \"Bracken\" : parser_bracken,\n",
    "               \"truth\" : parse_simulated_fasta}\n",
    "\n",
    "def parse_dir(results_directory, test_directory, parsers=PARSERS_MAP):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        This function parses all of the result directories including\n",
    "        the test directory and returns a data structure containing the results.\n",
    "        \n",
    "    Input:\n",
    "        1. Path to a singular simulated fasta file\n",
    "        \n",
    "    Output:\n",
    "        Abundance Dictionary containing:\n",
    "            1. taxid abundances\n",
    "            abundances_dict = {'tax_id_1' : 33,\n",
    "                               'tax_id_2' : 33,\n",
    "                               ...,\n",
    "                               'tax_id_N' : 33,\n",
    "                              }\n",
    "    \"\"\"\n",
    "    results = {} # final structure\n",
    "    \n",
    "    # step 1: parse truth\n",
    "    results = parse_results_directory(test_directory, results, parsers)\n",
    "    \n",
    "    # step 2: parse results\n",
    "    for tool_name in os.listdir(results_directory):\n",
    "        results_path = Path(results_directory) / Path(tool_name)\n",
    "        results = parse_results_directory(results_path, results, parsers, tool_name=tool_name)\n",
    "    \n",
    "    # step 3: save raw data in csv\n",
    "#     print(results)\n",
    "    \n",
    "    # create results from parsed information\n",
    "    results = parsed_dict_to_results(results)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "def parse_results_directory(test_directory, results: Dict, parsers, tool_name='true'):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        This function parses the truth directory, adding the parsed \n",
    "        abundance results to input results dictionary.\n",
    "        \n",
    "                    TEST\n",
    "                      |\n",
    "                    /   \\\n",
    "                TEST_1    TEST_2\n",
    "               /   |        |    \\\n",
    "       test-1.a test-1.b test-2.a test-2.b\n",
    "       \n",
    "                    into \n",
    "                       \n",
    "                   Results (a dictionary)\n",
    "                      |\n",
    "                    /   \\\n",
    "             \"TEST_1\"    \"TEST_2\"\n",
    "               /   |        |    \\\n",
    "    trueAb-1.a trueAb-1.b trueAb-2.a trueAb-2.b\n",
    "         \n",
    "    Input:\n",
    "        1. Path to a the entire truth test directory\n",
    "        2. results dictionary\n",
    "        3. parsers\n",
    "        \n",
    "    Output:\n",
    "        results dictionary with the true abundance values added:\n",
    "        \n",
    "        example:\n",
    "        'num_genomes_test': {\n",
    "                             '5_genomes_sim_illumina': {'true': \n",
    "                                  {'taxid_abundance': {'2886930': 0.2, \n",
    "                                                       '10868': 0.2, \n",
    "                                                       '2681618': 0.2, \n",
    "                                                       '10658': 0.2, \n",
    "                                                       '127507': 0.2}\n",
    "                                  }\n",
    "                             }\n",
    "\n",
    "    \"\"\"\n",
    "    # parse truth\n",
    "    for test_directory_name in os.listdir(test_directory):\n",
    "        test_full_path = Path(test_directory) / Path(test_directory_name) \n",
    "        if os.path.isdir(test_full_path): # if directory [should be directory]\n",
    "            # create sub dictionary if not exists\n",
    "            if test_directory_name not in results: results[test_directory_name] = {}\n",
    "            # loop through all fasta files and parse for true abundance\n",
    "            for file_test in os.listdir(test_full_path):\n",
    "                file_full_path = Path(test_full_path) / Path(file_test)\n",
    "                if os.path.isfile(file_full_path) and (file_full_path.suffix == \".fa\"): # if file and fasta\n",
    "                    # create subsub dictionary if not exists\n",
    "                    if file_test not in results[test_directory_name]:\n",
    "                        file_test = file_test.replace(\".fa\", \"\") # get name from file\n",
    "                        results[test_directory_name][file_test] = {}\n",
    "                        \n",
    "                    # add abundances to results\n",
    "                    abundances = parse_simulated_fasta(file_full_path)\n",
    "                    # update the results dictionary\n",
    "                    results[test_directory_name][file_test][\"truth\"] = abundances\n",
    "                \n",
    "                if os.path.isdir(file_full_path): # is results dir\n",
    "                    # add abundances to results\n",
    "                    try:\n",
    "                        abundances = parsers[tool_name](file_full_path) \n",
    "                        results[test_directory_name][file_test.strip(\".\")][tool_name] = abundances\n",
    "                    except:\n",
    "                        print(f\"{tool_name} is missing {file_test} from {test_directory_name}\")\n",
    "                        continue\n",
    "    return results\n",
    "\n",
    "def parsed_dict_to_results(results):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        This function parses the parsed results dictionary,\n",
    "        and creates a dictionary with all of the correct results.\n",
    "        Overall this utilizes the results structure.\n",
    "         \n",
    "    Input:\n",
    "        1. Path to a results dictionary containing truth and \n",
    "           tool predictions\n",
    "        \n",
    "    Output:\n",
    "        1. results dictionary with the tool results.\n",
    "    \"\"\"\n",
    "    for test_name in results.keys():\n",
    "        for sub_test in results[test_name].keys():\n",
    "            for tool_name, result_val in results[test_name][sub_test].items():\n",
    "                if tool_name != \"truth\":\n",
    "                    results[test_name][sub_test][tool_name] = get_results(results[test_name][sub_test][tool_name], \n",
    "                                                                          results[test_name][sub_test][\"truth\"])\n",
    "            del results[test_name][sub_test][\"truth\"]\n",
    "    return results\n",
    "\n",
    "def get_results(predicted_results, true_results):\n",
    "    \"\"\"                          \n",
    "    Description:\n",
    "        Given the true results and tool results, this\n",
    "        function returns a Result Struct for a given tool.\n",
    "        \n",
    "    **NOTES**: \n",
    "            This is where all of the metrics are calculated\n",
    "        so this is the most important function for the \n",
    "        benchmarking - contains the logic.\n",
    "         \n",
    "    Input:\n",
    "        1. dictionary of predicted results\n",
    "        2. dictionary of true results\n",
    "        \n",
    "    Output:\n",
    "        1. ResultStruct() with the tool results.\n",
    "    \"\"\"\n",
    "    # init\n",
    "    results_structure = ResultStruct()\n",
    "    predicted_abundances = predicted_results['taxid_abundance']\n",
    "    true_abundances = true_results['taxid_abundance']\n",
    "    \n",
    "    # error handling\n",
    "    if (len(true_abundances) == 0): \n",
    "        print(f\"TRUTH EMPTY\")\n",
    "        return results_structure\n",
    "    \n",
    "    # classification results\n",
    "    ## true positives\n",
    "    true_positives = []\n",
    "    for pred_taxid in predicted_abundances.keys():\n",
    "        if pred_taxid in true_abundances.keys():\n",
    "            true_positives.append(pred_taxid)\n",
    "    ## false positives\n",
    "    false_positives = []\n",
    "    for pred_taxid in predicted_abundances.keys():\n",
    "        if (pred_taxid not in true_abundances.keys()) and (pred_taxid != \"UNK\"):\n",
    "            false_positives.append(pred_taxid)\n",
    "    ## false negatives\n",
    "    false_negatives = []\n",
    "    for pred_taxid in true_abundances.keys():\n",
    "        if pred_taxid not in predicted_abundances.keys():\n",
    "            false_negatives.append(pred_taxid)\n",
    "            \n",
    "    ## get metrics\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "    \n",
    "    # abundance results\n",
    "    ## L2 distances\n",
    "    l2_dist_array = []\n",
    "    total_abundance_wo_unk = sum([abundance for key, abundance in predicted_abundances.items() if key != \"UNK\" ])\n",
    "    for tax_id, pred_abundance in predicted_abundances.items():\n",
    "        if tax_id in true_abundances:\n",
    "            pred_abundance /= total_abundance_wo_unk\n",
    "            l2_diff = abs(pred_abundance - true_abundances[tax_id])**2\n",
    "        else:\n",
    "            l2_diff = 0\n",
    "        l2_dist_array.append(l2_diff)\n",
    "    \n",
    "    # store into results structure\n",
    "    results_structure.l2_abundance_distance = l2_dist_array\n",
    "    results_structure.classification_recall = recall\n",
    "    results_structure.classification_precision = precision\n",
    "    \n",
    "    return results_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO CSV\n",
    "\n",
    "This section stores the data as a csv file to be used by a second notebook for plotting. This is done to allow parsing on a cluster and plotting locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 1.16102\n",
      "FastViromeExplorer is missing 800_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k75m31 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k35m31 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k28m28 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_1000000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_800000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_900000_reads from num_reads_800genomes\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.00499\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 2.26741\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 1.16361\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.01201\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.01201\n",
      "EnrichSeq_k28m28 is missing 800_genomes_1000000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_800000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_900000_reads from num_reads_800genomes\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_041918.1\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 1.16102\n",
      "FastViromeExplorer is missing 800_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k75m31 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k35m31 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k28m28 is missing 10_genomes_500000_reads from num_genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_1000000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_800000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_900000_reads from num_reads_800genomes\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "\tnumber of true NOT found (fine if 1): 1\n",
      "\tNCBI ID: NC_052982.1\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.00499\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 2.26741\n",
      "ERROR:\n",
      "\tFVE: NC_029069.1 NOT CONVERTED\n",
      "\tabundances: 1.16361\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.01201\n",
      "ERROR:\n",
      "\tFVE: NC_029072.1 NOT CONVERTED\n",
      "\tabundances: 1.01201\n",
      "EnrichSeq_k28m28 is missing 800_genomes_1000000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_800000_reads from num_reads_800genomes\n",
      "EnrichSeq_k28m28 is missing 800_genomes_900000_reads from num_reads_800genomes\n"
     ]
    }
   ],
   "source": [
    "benchmarking_directories = [\n",
    "                             [\"//scratch/summit/dral3008/benchmarking_enrichseq/RESULTS/results-NEW/test_1/\",\n",
    "                              \"//scratch/summit/dral3008/benchmarking_enrichseq/TESTS/tests_new/test_1/\"],\n",
    "                             [\"//scratch/summit/dral3008/benchmarking_enrichseq/RESULTS/results-NEW/test_2/\",\n",
    "                              \"//scratch/summit/dral3008/benchmarking_enrichseq/TESTS/tests_new/test_2/\"],\n",
    "                             [\"//scratch/summit/dral3008/benchmarking_enrichseq/RESULTS/results-NEW/test_3/\",\n",
    "                              \"//scratch/summit/dral3008/benchmarking_enrichseq/TESTS/tests_new/test_3/\"],\n",
    "                            ]\n",
    "\n",
    "def create_results_csv(benchmarking_directories=benchmarking_directories, resultsfile_name=\"results_file.csv\"):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This method prints out the results and saves them to a CSV for \n",
    "        plotting and further analysis.\n",
    "    Note:\n",
    "        1. Careful - it rewrites.\n",
    "        2. There are messages printed out to help troubleshooting/validation.\n",
    "    \"\"\"\n",
    "    global ENRICHSEQ_OUTPUTILE\n",
    "    verbose = False\n",
    "    with open(resultsfile_name, 'w') as results_file:\n",
    "        results_file.write(f\"replicate_ID,MergeOverlap,major_test,sub_test,tool,genome_count,read_count,classification_recall,classification_precision,l2_abundance_distance\\n\")\n",
    "        if verbose:\n",
    "            print(f\"replicate_ID,MergeOverlap,major_test,sub_test,tool,genome_count,read_count,classification_recall,classification_precision,l2_abundance_distance\")\n",
    "        for outfile_bool in [1,0]:\n",
    "            ENRICHSEQ_OUTPUTILE = outfile_bool\n",
    "            for iteration, test_results in enumerate(benchmarking_directories):\n",
    "                results = parse_dir(test_results[0], test_results[1])\n",
    "                for major_test in results.keys(): # ['num_genomes', 'config_files', 'num_reads_800genomes', 'num_reads_200genomes']\n",
    "                    for sub_test in results[major_test].keys():\n",
    "                        for tool, result in results[major_test][sub_test].items():\n",
    "                            genomes = sub_test.split(\"genomes\")[0].strip(\"_\")\n",
    "                            reads = sub_test.split(\"reads\")[0].split(\"_\")[-2]\n",
    "                            results_file.write(f\"{iteration},{outfile_bool},{major_test},{sub_test},{tool},{genomes},{reads},{result.classification_recall},{result.classification_precision},{np.average(result.l2_abundance_distance)}\\n\")\n",
    "                            if verbose:\n",
    "                                print(f\"{iteration},{outfile_bool},{major_test},{sub_test},{tool},{genomes},{reads},{result.classification_recall},{result.classification_precision},{np.average(result.l2_abundance_distance)}\")\n",
    "\n",
    "create_results_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
